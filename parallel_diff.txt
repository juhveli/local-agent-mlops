diff --git a/apps/deep_research/agent.py b/apps/deep_research/agent.py
index 1a3f91f..2c49273 100644
--- a/apps/deep_research/agent.py
+++ b/apps/deep_research/agent.py
@@ -12,6 +12,7 @@ sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspa
 from openai import OpenAI
 from opentelemetry import trace
 from tavily import TavilyClient
+from duckduckgo_search import DDGS
 from dotenv import load_dotenv

 load_dotenv()
@@ -22,6 +23,22 @@ register(project_name="deep-research-agent", endpoint="http://localhost:6006/v1/

 tracer = trace.get_tracer("deep_research_agent")

+HIGH_AUTHORITY_DOMAINS = [
+    "wikipedia.org",
+    "bbc.com",
+    "cnn.com",
+    "reuters.com",
+    "nytimes.com",
+    "washingtonpost.com",
+    "theguardian.com",
+    "npr.org",
+    "bloomberg.com",
+    "forbes.com",
+    "wsj.com",
+    "cnbc.com",
+    "gov",
+    "edu",
+]

 class DeepResearchAgent:
     """
@@ -110,28 +127,73 @@ Output ONLY a JSON array of search query strings, nothing else. Example: ["query
         # Fallback: return original query plus a general version
         return [query, query.split("?")[0] if "?" in query else query]

-    @tracer.start_as_current_span("tavily_search")
-    def search_web(self, query: str, num_results: int = 3) -> List[Dict[str, str]]:
-        """Search using Tavily API."""
+    @tracer.start_as_current_span("search_web")
+    def search_web(self, query: str, num_results: int = 3, provider: str = "tavily", search_depth: str = "basic", include_domains: List[str] = []) -> List[Dict[str, str]]:
+        """Search using Tavily API or DuckDuckGo."""
         span = trace.get_current_span()
         span.set_attribute("search.query", query)
+        span.set_attribute("search.provider", provider)
+        span.set_attribute("search.depth", search_depth)

+        results = []
         try:
-            response = self.tavily.search(query=query, max_results=num_results, include_raw_content=True)
-
-            results = []
-            for item in response.get("results", []):
-                results.append({
-                    "title": item.get("title", ""),
-                    "url": item.get("url", ""),
-                    "content": item.get("raw_content") or item.get("content", ""),
-                    "query": query
-                })
+            if provider == "duckduckgo":
+                with DDGS() as ddgs:
+                    # DDGS doesn't support 'include_raw_content' directly same way, but returns body
+                    # It also doesn't support easy domain filtering in the API call itself for list of domains usually,
+                    # but we can try site: if include_domains is small, or filter post-hoc.
+                    # For stability, we'll post-filter for DDGS if list is long.
+
+                    # If high authority domains are requested and it's a small list, we could append to query,
+                    # but let's do post-filtering for consistent behavior across large lists.
+
+                    ddg_results = list(ddgs.text(query, max_results=num_results * 2)) # Fetch more for filtering
+
+                    for item in ddg_results:
+                        results.append({
+                            "title": item.get("title", ""),
+                            "url": item.get("href", ""),
+                            "content": item.get("body", ""),
+                            "query": query
+                        })

+            else: # Tavily (Default)
+                tavily_params = {
+                    "query": query,
+                    "max_results": num_results,
+                    "include_raw_content": True,
+                    "search_depth": search_depth
+                }
+
+                if include_domains:
+                    tavily_params["include_domains"] = include_domains
+
+                response = self.tavily.search(**tavily_params)
+
+                for item in response.get("results", []):
+                    results.append({
+                        "title": item.get("title", ""),
+                        "url": item.get("url", ""),
+                        "content": item.get("raw_content") or item.get("content", ""),
+                        "query": query
+                    })
+
+            # Post-processing filter for domains (essential for DDGS, optional but safe for Tavily)
+            if include_domains:
+                filtered_results = []
+                for res in results:
+                    url = res["url"]
+                    if any(d in url for d in include_domains):
+                        filtered_results.append(res)
+                results = filtered_results[:num_results]
+            else:
+                results = results[:num_results]
+
             span.set_attribute("search.num_results", len(results))
             return results
         except Exception as e:
             span.set_attribute("search.error", str(e))
+            print(f"Search Error ({provider}): {e}", file=sys.stderr)
             return []

     @tracer.start_as_current_span("generate_embedding")
@@ -269,7 +331,7 @@ ANSWER:"""
         return answer

     @tracer.start_as_current_span("deep_research")
-    async def research(self, query: str, max_iterations: int = 3) -> Dict[str, Any]:
+    async def research(self, query: str, max_iterations: int = 3, provider: str = "tavily", search_depth: str = "basic", include_domains: List[str] = []) -> Dict[str, Any]:
         """
         Execute iterative deep research:
         1. Decompose query into sub-queries
@@ -280,44 +342,47 @@ ANSWER:"""
         """
         span = trace.get_current_span()
         span.set_attribute("research.query", query)
+        span.set_attribute("research.provider", provider)

         all_sources = []
         searched_queries = set()

+        # Determine domains to use
+        # If include_domains is explicitly passed, use it.
+        # If it's passed as ["high_authority"] (special flag from UI perhaps?), use the preset.
+        # But looking at requirements, we should probably handle the preset mapping here or in main.
+        # Let's assume the caller passes the list if they want filtering.
+        target_domains = include_domains
+        if include_domains and "HIGH_AUTHORITY" in include_domains:
+             target_domains = HIGH_AUTHORITY_DOMAINS
+
         # Step 1: Decompose query
         queries = self.decompose_query(query)

         for iteration in range(max_iterations):
             span.set_attribute(f"research.iteration_{iteration}_queries", len(queries))

-            # Step 2: Search each query in parallel
-            search_tasks = []
-            new_queries = []
-
+            # Step 2: Search each query
             for q in queries:
                 if q in searched_queries:
                     continue
                 searched_queries.add(q)
-                new_queries.append(q)
-                search_tasks.append(asyncio.to_thread(self.search_web, q, num_results=3))
-
-            if search_tasks:
-                results_list = await asyncio.gather(*search_tasks)
-
-                # Process new results
-                new_sources = []
-                for q, results in zip(new_queries, results_list):
-                    for r in results:
-                        r["query"] = q
-                    new_sources.extend(results)

-                all_sources.extend(new_sources)
+                results = self.search_web(
+                    q,
+                    num_results=3,
+                    provider=provider,
+                    search_depth=search_depth,
+                    include_domains=target_domains
+                )
+                for r in results:
+                    r["query"] = q
+                all_sources.extend(results)

-                # Step 3: Store ONLY NEW sources in NornicDB
-                # Optimization: Only process newly found sources to avoid redundant embeddings
-                for src in new_sources:
-                    if src.get("content"):
-                        self.store_knowledge(src["content"][:5000], {"url": src["url"], "query": query})
+            # Step 3: Store sources in NornicDB
+            for src in all_sources:
+                if src.get("content"):
+                    self.store_knowledge(src["content"][:5000], {"url": src["url"], "query": query})

             # Step 4: Check if we have enough relevant sources
             if len(all_sources) >= 5:
